{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff7c2f-e39c-4bf2-8835-638eb6d485fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install PyGithub -q\n",
    "from github import Github\n",
    "import pandas as pd,numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Function to load datasets\n",
    "def load_dataset(base_url, dataset_names):\n",
    "    \"\"\"Loads multiple datasets from a base URL into a dictionary.\"\"\"\n",
    "    data = {}\n",
    "    for name in dataset_names:\n",
    "        try:\n",
    "            url = f\"{base_url}/{name}.csv\"\n",
    "            data[name] = pd.read_csv(url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "    return data\n",
    "\n",
    "# Base URLs and dataset names\n",
    "paris2024_base_url = \"https://raw.githubusercontent.com/Katyayani09/Datasets/main/azure_projects/olympics_data/paris2024/raw_data\"\n",
    "datasets_paris = [\"athletes\", \"coaches\", \"medals\", \"teams\"]\n",
    "\n",
    "# Load datasets\n",
    "paris_data = load_dataset(paris2024_base_url, datasets_paris)\n",
    "\n",
    "# Process datasets\n",
    "def process_athletes_data(df):\n",
    "    df = df[['name', 'gender', 'country_long', 'disciplines']].rename(\n",
    "        columns={'name': 'PersonName', 'gender': 'Gender', 'country_long': 'Country', 'disciplines': 'Discipline'})\n",
    "    df['Discipline'] = df['Discipline'].astype(str).str.replace(r'[\\[\\]\\']', '', regex=True)\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def process_coaches_data(df):\n",
    "    df = df[['name', 'country_long', 'disciplines', 'events']].rename(\n",
    "        columns={'name': 'Name', 'country_long': 'Country', 'disciplines': 'Discipline', 'events': 'Event'})\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def process_medals_data(df):\n",
    "    total_medals = df.groupby('country_long')['medal_type'].count().reset_index().rename(\n",
    "        columns={'medal_type': 'Total', 'country_long': 'TeamCountry'})\n",
    "    total_medals['Rank'] = np.arange(1, len(total_medals) + 1)\n",
    "    \n",
    "    medal_counts = df.groupby(['country_long', 'medal_type']).size().unstack(fill_value=0).reset_index()\n",
    "    medal_counts.columns = ['TeamCountry', 'Bronze', 'Gold', 'Silver']\n",
    "    \n",
    "    merged = total_medals.merge(medal_counts, on='TeamCountry')\n",
    "    merged['Rank by Total'] = merged['Total'].rank(method='min', ascending=False).astype(int)\n",
    "    \n",
    "    return merged[['Rank', 'TeamCountry', 'Gold', 'Silver', 'Bronze', 'Total', 'Rank by Total']]\n",
    "\n",
    "def process_teams_data(df):\n",
    "    return df[['team', 'discipline', 'country_long', 'events']].rename(\n",
    "        columns={'team': 'TeamName', 'discipline': 'Discipline', 'country_long': 'Country', 'events': 'Event'})\n",
    "\n",
    "def process_gender_data(df):\n",
    "    result = (\n",
    "        df.groupby(['Discipline', 'Gender'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reset_index()\n",
    "        .rename_axis(None, axis=1))\n",
    "    result['Total'] = result['Female'] + result['Male']\n",
    "    result.columns = ['Discipline', 'Female', 'Male', 'Total']\n",
    "    return result\n",
    "    \n",
    "def process_athletes_data2(df):\n",
    "    df = df[['PersonName', 'Country', 'Discipline']]\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "# Clean datasets\n",
    "a2_cleaned = process_athletes_data(paris_data['athletes'])\n",
    "c2_cleaned = process_coaches_data(paris_data['coaches'])\n",
    "m2_cleaned = process_medals_data(paris_data['medals'])\n",
    "t2_cleaned = process_teams_data(paris_data['teams'])\n",
    "e2_cleaned = process_gender_data(a2_cleaned)\n",
    "a2_cleaned =a2_cleaned[['PersonName', 'Country', 'Discipline']]\n",
    "\n",
    "datasets_cleaned = {\n",
    "    'Athletes': a2_cleaned,\n",
    "    'Coaches': c2_cleaned,\n",
    "    'Medals': m2_cleaned,\n",
    "    'Teams': t2_cleaned,\n",
    "    'EntriesGender' : e2_cleaned\n",
    "}\n",
    "\n",
    "\n",
    "datasets_cleaned\n",
    "\n",
    "def upload_dataframes_to_github(repo, dataframes, base_path, commit_message=\"Add processed datasets\", branch=\"main\"):\n",
    "    \"\"\"\n",
    "    Uploads DataFrames directly to a GitHub repository.\n",
    "    \"\"\"\n",
    "    for name, df in dataframes.items():\n",
    "        try:\n",
    "            file_path = f\"{base_path}/{name.lower()}.csv\"\n",
    "            content = df.to_csv(index=False)\n",
    "            existing_file = None\n",
    "\n",
    "            # Check if the file already exists\n",
    "            try:\n",
    "                existing_file = repo.get_contents(file_path, ref=branch)\n",
    "            except Exception:\n",
    "                print(f\"{file_path} does not exist. Creating new file.\")\n",
    "\n",
    "            # Overwrite if the file exists, otherwise create a new file\n",
    "            if existing_file:\n",
    "                repo.update_file(file_path, commit_message, content, existing_file.sha, branch=branch)\n",
    "                print(f\"Overwritten: {file_path}\")\n",
    "            else:\n",
    "                repo.create_file(file_path, commit_message, content, branch=branch)\n",
    "                print(f\"Uploaded: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {name}: {e}\")\n",
    "\n",
    "\n",
    "# Fetch GitHub token from environment variable\n",
    "github_token = os.getenv(\"GITHUB_ACCOUNT\")\n",
    "if not github_token:\n",
    "    raise ValueError(\"GitHub token not found. Set the token as an environment variable.\")\n",
    "\n",
    "g = Github(github_token)\n",
    "\n",
    "# Access the repository\n",
    "repo_name = \"Katyayani09/Datasets\"\n",
    "try:\n",
    "    repo = g.get_repo(repo_name)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to access repository {repo_name}: {e}\")\n",
    "\n",
    "# Upload cleaned datasets to GitHub\n",
    "upload_dataframes_to_github(repo, datasets_cleaned, \"azure_projects/olympics_data/paris2024/processed_data\")\n",
    "#upload_dataframes_to_github(repo, datasets_cleaned, \"azure_projects/delete_data\")\n",
    "print(\"All files successfully uploaded to GitHub.\")\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2d448-d31e-4ae7-8bf6-e0344b89a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
